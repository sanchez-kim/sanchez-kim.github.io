{"0": {
    "doc": "ETC",
    "title": "ETC",
    "content": " ",
    "url": "/docs/Note/ETC/",
    
    "relUrl": "/docs/Note/ETC/"
  },"1": {
    "doc": "Gaze Estimation",
    "title": "아동의 시선 추적을 통한 자폐 진단",
    "content": ". | 아동에게 시각적 자극을 제시하고 Gaze(시선)을 추적 | 추적한 Gaze Point로 Scanpath 이미지 생성 | Scanpath 이미지로 자폐 유무를 분류 . | 시선 추적만으로 자폐 진단이 가능한가? ",
    "url": "/docs/%08AI/GazeEstimation/#%EC%95%84%EB%8F%99%EC%9D%98-%EC%8B%9C%EC%84%A0-%EC%B6%94%EC%A0%81%EC%9D%84-%ED%86%B5%ED%95%9C-%EC%9E%90%ED%8F%90-%EC%A7%84%EB%8B%A8",
    
    "relUrl": "/docs/%08AI/GazeEstimation/#아동의-시선-추적을-통한-자폐-진단"
  },"2": {
    "doc": "Gaze Estimation",
    "title": "ASD(Autism Spectrum Disorder) Detection",
    "content": ". 아동이 동영상 속 인물의 눈을 응시한 시간 비율 (출처:UNIST) . | . | . | 1~4세 미만 아동 616명에게 1분 미만의 짧은 영상을 보여주고 눈 운동 추적 장비를 통해 측정 | 자폐 아동들이 동영상 속 인물의 눈을 응시한 총 시간은 다른 집단과 유사했음. | 자폐 영유아들은 눈동자를 응시하지 못한다기보다 얼굴 전체를 덜 보는 경향이 있다. | 문맥에 맞게 중요한 정보로 주의 집중하는 능력이 일반인에 비해 떨어짐을 알 수 있음. | . ",
    "url": "/docs/%08AI/GazeEstimation/#asdautism-spectrum-disorder-detection",
    
    "relUrl": "/docs/%08AI/GazeEstimation/#asdautism-spectrum-disorder-detection"
  },"3": {
    "doc": "Gaze Estimation",
    "title": "†DeepLabCut, GazeNet을 이용한 eye-tracking",
    "content": "참고문서 . | Video camera-based eye-tracking . | Model-based . | 각막에 반사되는 적외선의 패턴으로 gaze의 포인트 계산 | . | Appearance-based . | 이미지 데이터에만 의존하여 눈의 음직임을 추적 | 머리가 고정되어 있지 않은 경우 시선을 추정하기 어렵다 | position of facial landmarks VS point of gaze | . | . | . ",
    "url": "/docs/%08AI/GazeEstimation/#deeplabcut-gazenet%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-eye-tracking",
    
    "relUrl": "/docs/%08AI/GazeEstimation/#deeplabcut-gazenet을-이용한-eye-tracking"
  },"4": {
    "doc": "Gaze Estimation",
    "title": "†Computer Vision Approach",
    "content": "GazeTracking OWLET dlib-models . | dlib으로 face landmark를 추출하고 눈의 움직임을 추척하는 방식 추후 dlib의 성능을 개선할 수 있을지? . | . ",
    "url": "/docs/%08AI/GazeEstimation/#computer-vision-approach",
    
    "relUrl": "/docs/%08AI/GazeEstimation/#computer-vision-approach"
  },"5": {
    "doc": "Gaze Estimation",
    "title": "†Without Calibration",
    "content": "GazeCapture . | Smart Phone, Tablet으로 부터 획득한 데이터를 바탕으로 Gaze Estimation | 얼굴과 좌우 눈 이미지, 그리고 25*25 face-grid를 입력값으로 넣고, 2D gaze point를 출력함. | 캘리브레이션이 필요없다고는 하나, 원래 논문에서 제시된 모델은 아이폰 및 아이패드 등의 모바일 디바이스 데이터를 기반으로 학습되었으므로 실제 커스텀 데이터셋을 적용하기에는 다소 어려움이 있다. (오프셋 설정이나 카메라 정보를 기반으로 캘리브레이션이 진행되어야함..) | . ",
    "url": "/docs/%08AI/GazeEstimation/#without-calibration",
    
    "relUrl": "/docs/%08AI/GazeEstimation/#without-calibration"
  },"6": {
    "doc": "Gaze Estimation",
    "title": "Gaze Estimation",
    "content": "RESEARCH . ON-PROGRESS . ",
    "url": "/docs/%08AI/GazeEstimation/",
    
    "relUrl": "/docs/%08AI/GazeEstimation/"
  },"7": {
    "doc": "Issues",
    "title": "생각날 때 꺼내먹으려고 기록하는 이슈 &amp; 팁 모음",
    "content": " ",
    "url": "/docs/Note/Issues/#%EC%83%9D%EA%B0%81%EB%82%A0-%EB%95%8C-%EA%BA%BC%EB%82%B4%EB%A8%B9%EC%9C%BC%EB%A0%A4%EA%B3%A0-%EA%B8%B0%EB%A1%9D%ED%95%98%EB%8A%94-%EC%9D%B4%EC%8A%88--%ED%8C%81-%EB%AA%A8%EC%9D%8C",
    
    "relUrl": "/docs/Note/Issues/#생각날-때-꺼내먹으려고-기록하는-이슈--팁-모음"
  },"8": {
    "doc": "Issues",
    "title": "†LINUX",
    "content": "로케일 설정 . | 도커 컨테이너 상의 bash에서 로케일 설정이 제대로 되지 않는 것을 발견. (한글 안쳐짐) | 사용 가능한 로케일 확인. | . locale -a . | 환경변수 설정 | . export LANG=ko_KR.utf8 bash . 이걸 도커에 넣어야할지 리눅스 이슈에 넣어야 할지 고민을 많이 했다만.. 추후 우분투에서 문제가 있을 경우도 있고해서 리눅스 이슈란에 포함시킴. GPG 키 에러 . | 도커로 이미지를 빌드할때 발생하곤 함. | 키를 등록해주면 해결. | 일단 업데이트 및 업그레이드를 하고 sudo를 설치해주자. | . apt update &amp;&amp; apt upgrade apt install sudo . | 이후 에러 메시지의 PUBKEY를 복사한 후 아래 커맨드로 등록을 해주면 끝 | . sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys &lt;PUBKEY&gt; . 프로세스 한방에 종료 시키기 . | 통상 kill -9 [PID]로 프로세스를 종료시키는데 만약에 병렬 프로세스가 걸려있을 경우 일일이 PID를 찾기가 귀찮아진다. | 아래 커맨드로 한방에 프로세스를 종료해보자. | . kill -9 -ef &lt;PROCESS_NAME&gt; . | 권한이 없다면 sudo를 넣어주자 | . wget으로 구글드라이브 파일 다운로드 받기 . wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id={FILEID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&amp;id={FILEID}\" -O {FILENAME} &amp;&amp; rm -rf /tmp/cookies.txt . | 직접적으로 drive.google.com으로 접근이 불가능하기 때문에 docs.google.com으로 우회해야 한다. | FILEID에 공유 링크에서 파일아이디를 복사하여 집어넣고 FILENAME에는 원하는 파일명을 작성하면 된다. | . ",
    "url": "/docs/Note/Issues/#linux",
    
    "relUrl": "/docs/Note/Issues/#linux"
  },"9": {
    "doc": "Issues",
    "title": "†DOCKER",
    "content": "컨테이너 재시작 . | 도커 컨테이너가 중지되었을때 강제로 무조건 다시 시작하도록 하는 커맨드. | . docker update --restart=always &lt;ContainerID&gt; . 컨테이너 저장하기 . | 작업하던 컨테이너 내용을 그대로 저장하고 싶을때, 갑자기 컨테이너를 내리거나 재시작해야할때! | . docker commit [CONTAINER] [IMAGENAME] . ",
    "url": "/docs/Note/Issues/#docker",
    
    "relUrl": "/docs/Note/Issues/#docker"
  },"10": {
    "doc": "Issues",
    "title": "†JUPYTER",
    "content": "Sublime Text 스타일로 키 맵핑하기 . | VSCode를 쓰다보면 cmd + D로 다중 선택하는 것에 익숙해져있다. | 주피터 노트북에서는 한줄을 다 지우는 단축키여서 화가 날때가 많다. | custom.js를 수정하면 키 맵핑이 가능하다. | . # 커스텀 파일 경로 # 없으면 하나 만들어주자 ~/.jupyter/custom/custom.js . | custom.js 코드 | . require([ \"codemirror/keymap/sublime\", \"notebook/js/cell\", \"base/js/namespace\", ], function (sublime_keymap, cell, IPython) { cell.Cell.options_default.cm_config.keyMap = \"sublime\"; var cells = IPython.notebook.get_cells(); for (var cl = 0; cl &lt; cells.length; cl++) { cells[cl].code_mirror.setOption(\"keyMap\", \"sublime\"); } }); . 테마설치 . pip install jupyterthemes pip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib nbextension install # 자주 쓰는 설정 1 jt -t onedork -T -N -kl -f roboto -fs 12 -tfs 11 -nfs 14 -tfs 14 -ofs 10 -cellw 90% -lineh 170 -cursc r -cursw 6 # 자주 쓰는 설정 2 jt -t chesterish -f bitstream -fs 12 -tf roboto -tfs 13 -nf opensans -nfs 12 -ofs 12 -dfs 12 -cellw 95% -lineh 150 -T -N . ",
    "url": "/docs/Note/Issues/#jupyter",
    
    "relUrl": "/docs/Note/Issues/#jupyter"
  },"11": {
    "doc": "Issues",
    "title": "†GIT",
    "content": "gitignore 적용 안될 때 . | 캐시를 삭제해주자 | . git rm -r --cached . ",
    "url": "/docs/Note/Issues/#git",
    
    "relUrl": "/docs/Note/Issues/#git"
  },"12": {
    "doc": "Issues",
    "title": "Issues",
    "content": " ",
    "url": "/docs/Note/Issues/",
    
    "relUrl": "/docs/Note/Issues/"
  },"13": {
    "doc": "Stable Diffusion",
    "title": "Stable Diffusion Web UI 설치 &amp; 사용법",
    "content": " ",
    "url": "/docs/%08AI/StableDiffusion/#stable-diffusion-web-ui-%EC%84%A4%EC%B9%98--%EC%82%AC%EC%9A%A9%EB%B2%95",
    
    "relUrl": "/docs/%08AI/StableDiffusion/#stable-diffusion-web-ui-설치--사용법"
  },"14": {
    "doc": "Stable Diffusion",
    "title": "설치방법",
    "content": ". | Ubuntu 20.04 환경을 기준으로 설치를 진행함 | 본인은 remote server의 도커 환경에 설치를 진행하였음 | 공식 repository에는 파이썬 3.10 설치하고 webui.sh 실행하면 끝이라고 되어있지만 그렇게 간단하지 않다.. | . ",
    "url": "/docs/%08AI/StableDiffusion/#%EC%84%A4%EC%B9%98%EB%B0%A9%EB%B2%95",
    
    "relUrl": "/docs/%08AI/StableDiffusion/#설치방법"
  },"15": {
    "doc": "Stable Diffusion",
    "title": "Dockerfile 만들기",
    "content": ". | 우선 공식 repo를 클론하고 해당 폴더에 도커파일을 작성해주자. | 도커파일의 내용은 아래와 같다. | . FROM nvidia/cuda:11.4.0-cudnn8-runtime-ubuntu20.04 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update &amp;&amp; apt-get upgrade -y \\ &amp;&amp; apt-get install -y git curl wget software-properties-common \\ &amp;&amp; echo \"Asia/Seoul\" &gt; /etc/timezone \\ &amp;&amp; apt-get install -y tzdata \\ &amp;&amp; rm /etc/localtime \\ &amp;&amp; ln -snf /usr/sahre/zoneinfo/Asia/Seoul /etc/localtime \\ &amp;&amp; dpkg-reconfigure -f noninteractive tzdata \\ &amp;&amp; add-apt-repository ppa:deadsnakes/ppa -y \\ &amp;&amp; apt-get install -y python3.10 python3.10-venv python3-pip \\ &amp;&amp; apt-get install -y libsm6 libxext6 libxrender-dev libgl1-mesa-glx libglib2.0-0 \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN apt-get update &amp;&amp; apt-get upgrade -y RUN apt-get install libgoogle-perftools4 libtcmalloc-minimal4 -y RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10 ENV PATH=\"/venv/bin:$PATH\" WORKDIR /webui . | 본인이 개삽질하면서 작성한 도커파일인데 테스트는 해보지 않았으나 아마도 잘 빌드 될 것임. | 뭘 설치해야하는지 들여다보면 대충 감이 올것임. | 위의 코드 내용으로 도커파일을 작성하고 이미지를 빌드하자. docker build --network=host -t webui:latest . 권한문제 발생시 . # this script cannot be run as root by default can_run_as_root=0 . | 초기에 webui실행시 root로는 실행할 수 없게끔 되어있다. | 다른 유저로 실행하거나 위에 해당하는 값을 1로 수정해주면 해결됨. | 아니면 더 간단하게 -f 옵션을 주어 관리자로 실행할 수 있다. | 빌드가 잘 되었다면 컨테이너를 만들어 접속하고, ui를 실행해보자./webui.sh --listen --deepdanbooru --enable-insecure-extension-access . launch.py의 –help를 통해 어떤 옵션이 있는지 확인할 수 있다. 원하는 옵션을 선택하고 위와 같이 webui 쉘스크립트를 실행해준다. | . ",
    "url": "/docs/%08AI/StableDiffusion/#dockerfile-%EB%A7%8C%EB%93%A4%EA%B8%B0",
    
    "relUrl": "/docs/%08AI/StableDiffusion/#dockerfile-만들기"
  },"16": {
    "doc": "Stable Diffusion",
    "title": "ControlNet",
    "content": ". | Stable Diffusion으로 이미지를 생성할 때 사용자가 생성과정을 더 세부적으로 제어할 수 있도록 하는 기술이며, 일반적으로 conditioning은 text prompt가 그 역할을 하게되는데, 그와 별개로 conditioning 요소를 하나 더 추가한다고 보면되겠다. | 위의 그림상 (a)가 Stable Diffusion의 U-Net block이고 (b)가 ControlNet이다. | (a)는 pretrained model의 파라미터를 복제하여 학습가능한 block과 잠긴 block을 만들고 이는 zero convolution 레이어와 연결된다. | trainable copy라고 되어있는 학습 가능한 block만 학습을 진행하게 되는데 이때 zero convolution을 통해 출력값이 0에서 시작해서 점점 학습되도록 한다. | ControlNet의 모델은 Canny, Openpose, Depth Map 등 여러가지가 있으며, 여러개의 모델을 한번에 적용할 수 도 있다. 갓 컨트롤넷! | . 설치 &amp; 적용방법 . 작성예정 . ",
    "url": "/docs/%08AI/StableDiffusion/#controlnet",
    
    "relUrl": "/docs/%08AI/StableDiffusion/#controlnet"
  },"17": {
    "doc": "Stable Diffusion",
    "title": "Lora",
    "content": "적용방법 . 학습방법 . ",
    "url": "/docs/%08AI/StableDiffusion/#lora",
    
    "relUrl": "/docs/%08AI/StableDiffusion/#lora"
  },"18": {
    "doc": "Stable Diffusion",
    "title": "Trouble Shooting",
    "content": "webui 실행 시 나타나는 오류 정리 예정 . | Cannot locate TCMalloc . | apt install libgoogle-perftools4 libtcmalloc-minimal4 -y | . | . Extension Install . Requirements . ",
    "url": "/docs/%08AI/StableDiffusion/#trouble-shooting",
    
    "relUrl": "/docs/%08AI/StableDiffusion/#trouble-shooting"
  },"19": {
    "doc": "Stable Diffusion",
    "title": "Stable Diffusion",
    "content": "RESEARCH . ON-PROGRESS . ",
    "url": "/docs/%08AI/StableDiffusion/",
    
    "relUrl": "/docs/%08AI/StableDiffusion/"
  },"20": {
    "doc": "TTS",
    "title": "TTS 프로젝트 개요",
    "content": ". | KSS(Korean Single Speaker)데이터셋으로 구현된 모델은 흔하게 찾아볼 수 있으나 남성 목소리에 대한 구현체는 찾아보기가 어려워서 직접 학습해보고 테스트해보려한다. | 데이터는 음성과 스크립트의 페어가 필요한데 좋은 데이터가 드물어서 서칭끝에 AI hub의 카이스트 오디오북 데이터셋을 활용하기로 결정. | 추후 한국어 TTS 개발을 위한 데이터셋 전처리 코드도 정리해볼 예정이다. 구현되어있는 코드가 대부분 kss 기준으로 되어있는데 개인적으로는 좀 구조가 불편하게 되어있어서 다른 데이터셋에 적용시키기가 좀 귀찮더라. | . ",
    "url": "/docs/%08AI/TTS/#tts-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EA%B0%9C%EC%9A%94",
    
    "relUrl": "/docs/%08AI/TTS/#tts-프로젝트-개요"
  },"21": {
    "doc": "TTS",
    "title": "FastSpeech2",
    "content": "FastSpeech2 의 모델 구조 . | Non-autogressive 방식의 TTS 모델. | 애초에 입력값이 텍스트 뿐이기 떄문에 하나의 텍스트로 여러가지 형태의 음성이 나올 수 있으므로 one-to-many mapping problem에 해당한다. | Variation Adaptor를 도입하면서 phoneme hidden sequence에 energy, pitch, duration 등 여러가지 정보를 학습에 활용할 수 있게 되었고 좀 더 정확한 speech를 예측할 수 있게 되었다. | 오리지널 버전은 미리 학습된 autoregressive 방식의 teacher model을 이용하는 것이 아니라 MFA(Montreal Forced Aligner) 방식으로 문장의 duration 정보를 강제 정렬하여 더 정확하게 duration을 예측한다. | . | | . | 강제 정렬 과정 | . ",
    "url": "/docs/%08AI/TTS/#fastspeech2",
    
    "relUrl": "/docs/%08AI/TTS/#fastspeech2"
  },"22": {
    "doc": "TTS",
    "title": "프로젝트 절차",
    "content": " ",
    "url": "/docs/%08AI/TTS/#%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%A0%88%EC%B0%A8",
    
    "relUrl": "/docs/%08AI/TTS/#프로젝트-절차"
  },"23": {
    "doc": "TTS",
    "title": "Data Processing",
    "content": "1. 오픈데이터 다운로드 . | AI Hub 카이스트 오디오북 데이터셋에 가면 다운로드 가능하다. | 여기서 남자 성우 중 한명을 선택하여 오디오파일과 스크립트를 정리하였다. | AI Hub에 올라가있는 데이터다보니 나름 잘 정리되어있지만 누락된 데이터도 있으니 꼭 확인을 꼼꼼하게 하길 바란다. | . 2. TextGrid 만들기 . | FastSpeech2를 학습시키기 위해서는 phoneme-utterance sequence간의 alignment 정보가 필요한데 이를 TextGrid라고 한다. | TextGrid는 MFA를 사용해서 추출할 수 있다. | MFA 설치는 공식 문서를 참고해서 설치하는 것이 가장 깔끔하다.. 개인적으로는 도커를 활용하였다. | TextGrid를 추출하기 위해서는 MFA 학습용 데이터셋을 만들어야하는데 이때 원본 오디오파일과 그 오디오파일에 해당하는 스크립트, 그리고 음소(phoneme) 단위의 딕셔너리가 필요하다. | . | | . | kss 데이터셋을 음소 단위로 변환한 예 | . | 딕셔너리 제작시에 MFA로 g2p 모델을 학습시켜서 변환시키는 방법도 있지만 학습시간이 너무 오래걸리기 때문에 되도록이면 g2pk를 사용해서 미리 한글을 위의 그림처럼 phoneme으로 변환시켜 놓자. | 아래의 커맨드를 통해 최종적으로 TextGrid를 획득할 수 있다. | . mfa train [dataset_path] [dictionary_path] [save_model_path] . | dataset_path : 오디오, 스크립트 쌍(wav, lab)이 존재하는 디렉토리의 경로 | dictionary_path : g2pk로 만든 딕셔너리 파일의 경로 | save_model_path : 학습 완료 후 모델을 저장할 경로 | . mfa align [dataset_path] [dict_file_path] [model_path] [result_path] . | dataset_path : 학습 시킬 때 사용했던 데이터 경로와 동일 | dict_file_path : train 결과롤 생성된 *.dict 파일의 경로 | model_path : 학습된 모델 파일의 경로 | result_path : TextGrid를 저장할 경로 | . | | . | 생성된 TextGrid 예시 | . 3. Preprocessing . | TextGrid를 만들었다고 끝난게 아니다. 이제시작이다.. | TextGrid가 준비되었으면 데이터셋의 형태에 맞게 (kss데이터셋의 포맷을 따라가는게 편리함) 전처리를 진행하자. | . | | . | 전처리 후 폴더구조 | . ",
    "url": "/docs/%08AI/TTS/#data-processing",
    
    "relUrl": "/docs/%08AI/TTS/#data-processing"
  },"24": {
    "doc": "TTS",
    "title": "Training",
    "content": "| | . | 학습결과 | . | 보코더는 한국인 남성의 목소리를 학습시킨 모델이 없어서 VocGAN의 영어 다화자 사전학습 모델을 사용하였다. | 추후 diffwave 기반 모델 테스트 예정 | . | 학습용 데이터의 수량은 training set 9,115건, validation set 1,951건으로 결코 많은 양은 아닌 듯 하다. | 하이퍼파라미터가 굉장히 많은데(attention head, dropout, learning rate 등등), 하나하나 테스트해보기에는 너무 시간이 오래걸리므로 HGU-DLLAB의 Jackson Kang님의 코드에 수록되어있는 파라미터를 대부분 그대로 적용하였다. | 3000 epoch으로 총 180만 스텝을 학습시키게 되었다. | . 학습결과 분석 . | 데이터의 글자 빈도 수에 따라서 발화가 결정되는 케이스 존재 \\(\\rightarrow\\) overfitting 우짜꼬.. | loss 만으로는 학습이 잘 되었는지 판별이 불가. | 데이터 전처리에 따라서 품질이 달라질 수 있을듯.. | . ",
    "url": "/docs/%08AI/TTS/#training",
    
    "relUrl": "/docs/%08AI/TTS/#training"
  },"25": {
    "doc": "TTS",
    "title": "References",
    "content": ". | ming024’s implementation | HGU-DLLAB’s implementation for Korean | . ",
    "url": "/docs/%08AI/TTS/#references",
    
    "relUrl": "/docs/%08AI/TTS/#references"
  },"26": {
    "doc": "TTS",
    "title": "TTS",
    "content": "RESEARCH . ON-PROGRESS . ",
    "url": "/docs/%08AI/TTS/",
    
    "relUrl": "/docs/%08AI/TTS/"
  },"27": {
    "doc": "Talking Face Generation",
    "title": "Wav2Lip",
    "content": ". | 입술 모양을 생성하여 시각적으로 입술의 움직임을 음성신호에 맞추는 LipGAN에서 제안되었던 기술을 개선한 알고리즘 | LipGAN은 정적인 이미지에서는 높은 성능을 보여주었으나 실제 영상에 적용했을 때 시각적 이상 현상이 발생하거나 움직임이 자연스럽지 않았음 | Wav2Lip은 립싱크 학습과 시각적 결과를 생성하는 학습으로 분리하는 접근 방식을 제안 | SyncNet의 구조를 대부분 가져온듯함 | . ",
    "url": "/docs/%08AI/TalkingFaceGeneration/#wav2lip",
    
    "relUrl": "/docs/%08AI/TalkingFaceGeneration/#wav2lip"
  },"28": {
    "doc": "Talking Face Generation",
    "title": "Talking Face Generation",
    "content": "RESEARCH . ON-PROGRESS . ",
    "url": "/docs/%08AI/TalkingFaceGeneration/",
    
    "relUrl": "/docs/%08AI/TalkingFaceGeneration/"
  },"29": {
    "doc": "Home",
    "title": "Welcome to my blog!",
    "content": "print(\"hello, this is Sanchez\") . ",
    "url": "/#welcome-to-my-blog",
    
    "relUrl": "/#welcome-to-my-blog"
  },"30": {
    "doc": "Home",
    "title": "Notice",
    "content": "I’m Sanchez from Korea and I’m working as a data scientist. I believe in sharing knowledge, ideas, and creative content with others in a collaborative and inclusive manner. As part of commitment to fostering a culture of open sharing while respecting intellectual property rights, I have chosen to license my blog content under the Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license. ",
    "url": "/#notice",
    
    "relUrl": "/#notice"
  },"31": {
    "doc": "Home",
    "title": "What does this mean?",
    "content": "By licensing my blog under CC BY-NC-SA, I grant you the freedom to: . | Share: You are free to copy and redistribute my blog posts in any medium or format. | Adapt: You are allowed to remix, transform, and build upon my blog posts, creating derivative works based on them. | . However, there are a couple of important conditions to keep in mind: . | Attribution: If you use or share my blog posts, you must provide appropriate credit by clearly mentioning the author and linking back to the original blog post. | NonCommercial: My content cannot be used for commercial purposes. You may not generate revenue directly from my blog posts without obtaining explicit permission. | ShareAlike: If you remix, transform, or build upon my blog posts, you must distribute the resulting work under the same or a similar license as the original. | . I believe that this licensing approach encourages collaboration, knowledge dissemination, and the advancement of ideas while maintaining a balance between open sharing and respecting the rights of content creators. Please note that this licensing arrangement applies specifically to the textual content of my blog posts and does not extend to external resources or materials referenced within my articles. These may be subject to separate licensing agreements or copyright restrictions. If you have any questions regarding the licensing or would like to seek permission for commercial use of my content, please feel free to contact me. Thank you for visiting my blog and joining me on this journey of open knowledge sharing. ",
    "url": "/#what-does-this-mean",
    
    "relUrl": "/#what-does-this-mean"
  },"32": {
    "doc": "Home",
    "title": "Reference",
    "content": "이 블로그는 Just the Docs와 utterances.es를 사용하여 제작하였습니다. ",
    "url": "/#reference",
    
    "relUrl": "/#reference"
  },"33": {
    "doc": "Home",
    "title": "Home",
    "content": "DATA SCIENCE . COMPUTER VISION . NLP . DEEP LEARNING . ",
    "url": "/",
    
    "relUrl": "/"
  },"34": {
    "doc": "AI Research",
    "title": "Task 별 AI Research",
    "content": " ",
    "url": "/AI/Research#task-%EB%B3%84-ai-research",
    
    "relUrl": "/AI/Research#task-별-ai-research"
  },"35": {
    "doc": "AI Research",
    "title": "AI Research",
    "content": " ",
    "url": "/AI/Research",
    
    "relUrl": "/AI/Research"
  },"36": {
    "doc": "Note",
    "title": "끄적끄적",
    "content": " ",
    "url": "/Note#%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81",
    
    "relUrl": "/Note#끄적끄적"
  },"37": {
    "doc": "Note",
    "title": "Note",
    "content": " ",
    "url": "/Note",
    
    "relUrl": "/Note"
  }
}
